import os
import time
import logging
import base64
from google import genai
from google.genai import types
from google.genai.errors import ServerError, ClientError
import telegramify_markdown
from config.settings import GEMINI
from telebot.types import InlineKeyboardMarkup, InlineKeyboardButton

log = logging.getLogger(__name__)

# =========================
# Configura√ß√µes gerais
# =========================

BASE_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
INSTRUCOES_FILE = os.path.join(BASE_PATH, "ia", "instrucoes_gemini.txt")

# modelos em ordem de prioridade
MODELOS = [
    "gemini-3-flash-preview",  # r√°pido (preview, pode falhar)
    "gemini-2.5-pro",          # est√°vel
    "gemini-2.0-pro",          # fallback final
]

# hist√≥rico curto por usu√°rio
chat_sessions = {}

# cache simples para perguntas repetidas
cache_respostas = {}

# client Gemini (API nova)
client = genai.Client(api_key=GEMINI)


# =========================
# Utilit√°rios
# =========================

def carregar_instrucoes():
    try:
        with open(INSTRUCOES_FILE, "r", encoding="utf-8") as f:
            return f.read().strip()
    except FileNotFoundError:
        log.warning("instrucoes_gemini.txt n√£o encontrado")
        return ""


def gerar_resposta(prompt: str) -> str:
    """
    Tenta gerar resposta usando modelos em fallback.
    """
    ultimo_erro = None

    for modelo in MODELOS:
        try:
            inicio = time.perf_counter()

            response = client.models.generate_content(
                model=modelo,
                contents=prompt,
            )

            duracao = time.perf_counter() - inicio
            log.info(f"Gemini respondeu com {modelo} em {duracao:.2f}s")

            return response.text.strip()

        except ServerError as e:
            # 503 / indisponibilidade tempor√°ria
            log.warning(f"Modelo {modelo} indispon√≠vel, tentando fallback")
            ultimo_erro = e
            continue

        except ClientError as e:
            # erro de configura√ß√£o (key, projeto, etc.)
            log.error("Erro de configura√ß√£o Gemini", exc_info=e)
            break

    raise RuntimeError("Nenhum modelo Gemini dispon√≠vel") from ultimo_erro


# =========================
# Handler principal
# =========================

def responder_com_ia(bot, message):
    texto_usuario = message.text.strip()
    user_id = message.from_user.id

    # filtros b√°sicos (reduz custo e lat√™ncia)
    # if not texto_usuario or len(texto_usuario) < 6:
    if not texto_usuario:
        return
    
    cumprimentos = {
        "oi": "üëã Ol√°! Posso te ajudar com treinos, ranking ou pace.",
        "ol√°": "üëã Ol√°! Quer registrar um treino ou ver o ranking?",
        "ola": "üëã Ol√°! Quer registrar um treino ou ver o ranking?",
        "bom dia": "üåÖ Bom dia! Bora correr hoje?",
        "boa tarde": "‚òÄÔ∏è Boa tarde! Como posso ajudar?",
        "boa noite": "üåô Boa noite! Quer ver seu desempenho?",
        "b dia": "üåÖ Bom dia! Bora correr hoje?",
    }

    chama_funcao = ("\n\n Se preferir temos uma lista de comandos dispon√≠veis\n Para acessar basta clicar no bot√£o abaixo üëá")

    if texto_usuario in cumprimentos:
        # bot.send_message(message.chat.id, cumprimentos[texto_usuario] + chama_funcao)
        markup = InlineKeyboardMarkup()
        markup.add(
            InlineKeyboardButton(
                "üìã Ver comandos",
                url="https://t.me/IMW_Runners_bot?start=menu"
            )
        )

        bot.send_message(
            message.chat.id,
            cumprimentos[texto_usuario] + chama_funcao,
            reply_markup=markup
        )
        return

    if len(texto_usuario) < 4:
        fake_message = message
        fake_message.text = "/start"
        bot.process_new_messages([fake_message])
        return

    if texto_usuario.startswith("/"):
        return

    # cache (resposta instant√¢nea)
    chave_cache = texto_usuario.lower()
    if chave_cache in cache_respostas:
        bot.send_message(
            message.chat.id,
            cache_respostas[chave_cache],
            parse_mode="MarkdownV2"
        )
        return

    bot.send_chat_action(message.chat.id, "typing")

    instrucoes = carregar_instrucoes()

    historico = chat_sessions.get(user_id, [])
    historico_resumido = "\n".join(historico[-2:])  # bem curto

    prompt = f"""
            {instrucoes}

            Hist√≥rico recente:
            {historico_resumido}

            Usu√°rio disse:
            {texto_usuario}
            """

    try:
        resposta = gerar_resposta(prompt)
    except RuntimeError:
        bot.send_message(
            message.chat.id,
            "‚ö†Ô∏è O assistente est√° temporariamente indispon√≠vel. Tente novamente em instantes."
        )
        return

    if not resposta:
        return

    # atualiza hist√≥rico (curto)
    historico.append(f"Usu√°rio: {texto_usuario}")
    historico.append(f"IA: {resposta}")
    chat_sessions[user_id] = historico[-4:]

    # salva cache
    cache_respostas[chave_cache] = telegramify_markdown.markdownify(resposta)

    bot.send_message(
        message.chat.id,
        cache_respostas[chave_cache],
        parse_mode="MarkdownV2"
    )


class GeminiClient:

    def __init__(self, api_key: str):
        self.client = genai.Client(api_key=api_key)

    # ======================================================
    # GERA√á√ÉO DE IMAGENS PARA POST (3 varia√ß√µes)
    # ======================================================

    def generate_images(
        self,
        prompt: str,
        images: list[str],
        n: int = 3,
        size: str = "1080x1920"
    ) -> list[bytes]:

        """
        prompt: texto final j√° montado pelo PostService
        images: lista de imagens em base64 (sem header data:image/...)
        n: quantidade de imagens a gerar
        size: resolu√ß√£o desejada (vertical)
        """

        contents = []

        # üîπ Adicionar imagens enviadas pelo usu√°rio
        for img_base64 in images:
            contents.append(
                types.Part.from_bytes(
                    data=base64.b64decode(img_base64),
                    mime_type="image/jpeg"
                )
            )

        # üîπ Adicionar instru√ß√£o textual forte
        prompt_final = f"""
        {prompt}

        Gere exatamente {n} imagens diferentes.
        Formato obrigat√≥rio: {size}.
        Orienta√ß√£o vertical.
        Cada imagem deve ter layout diferente.
        N√£o gere texto explicativo fora das imagens.
        """

        contents.append(prompt_final)

        # üîπ Chamada ao Gemini
        response = self.client.models.generate_content(
            model="gemini-2.0-flash-exp",  # modelo multimodal r√°pido
            contents=contents,
            config=types.GenerateContentConfig(
                response_modalities=["IMAGE"]
            )
        )

        imagens_bytes = []

        # üîπ Extrair imagens retornadas
        for candidate in response.candidates:
            for part in candidate.content.parts:
                if part.inline_data:
                    imagens_bytes.append(
                        base64.b64decode(part.inline_data.data)
                    )

        # Seguran√ßa: garantir 3 outputs
        if len(imagens_bytes) < n:
            raise Exception("Gemini n√£o retornou imagens suficientes.")

        return imagens_bytes[:n]